{"cells":[{"cell_type":"markdown","metadata":{"id":"vt5hanGL4WpD"},"source":["# Train a deep Bayesian CNN on XPS data"]},{"cell_type":"markdown","metadata":{"id":"uTjEnuR-LwEo"},"source":["In this notebook, we will train a deep probabilistic (Bayesian) convolutional network on XPS spectra made up of linear combinations of reference spectra.\n","\n","This notebook is currently used experimentally and cannot be run end-to-end without changing the code."]},{"cell_type":"markdown","metadata":{"id":"u6EVGrGFLwEr"},"source":["## Setup"]},{"cell_type":"markdown","metadata":{"id":"dU2aEkqdLwE_"},"source":["### Set up working directory and notebook path"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5iL11_yXLwFB"},"outputs":[],"source":["import os\n","\n","# Change paths according to your needs.\n","if os.getenv(\"COLAB_RELEASE_TAG\"):\n","    WORKING_DIR = \"/content/drive/My Drive/deepxps\"\n","    NOTEBOOK_PATH = \"/content/drive/My Drive/deepxps/xpsdeeplearning/notebooks/train_prob.ipynb\"\n","\n","else:\n","    WORKING_DIR = r\"C:/Users/pielsticker/Lukas/MPI-CEC/Projects/deepxps\"\n","    NOTEBOOK_PATH = r\"C:/Users/pielsticker/Lukas/MPI-CEC/Projects/deepxps/xpsdeeplearning/notebooks/train_prob.ipynb\"\n","\n","os.chdir(WORKING_DIR)"]},{"cell_type":"markdown","metadata":{},"source":["### Mount google drive"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# This cell only needs to be run when you are working on Google Colab.\n","# Mount drive\n","from google.colab import drive\n","drive.mount(\"/content/drive\")"]},{"cell_type":"markdown","metadata":{"id":"rC3yXYXaLwEt"},"source":["### Install packages and import modules"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s4MxYB_b33V9"},"outputs":[],"source":["%%capture\n","# Install packages if needed\n","if \"google.colab\" in str(get_ipython()):\n","     %pip install ./xpsdeeplearning\n","\n","# Import standard modules\n","import datetime\n","import numpy as np\n","import pytz\n","import importlib\n","import matplotlib.pyplot as plt\n","\n","# Magic commands\n","%matplotlib inline\n","from IPython.core.interactiveshell import InteractiveShell\n","InteractiveShell.ast_node_interactivity = \"all\"\n","\n","# Disable tf warnings\n","os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n","\n","# Import tensorflow\n","import tensorflow as tf\n","from tensorflow.keras import backend as K"]},{"cell_type":"markdown","metadata":{"id":"FWgepirGJ44v"},"source":["### Install and import TensorFlow and TensorFlow Probability"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tcFGRuOWJ4cX"},"outputs":[],"source":["# Disable tf warnings\n","os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n","\n","import tensorflow as tf\n","import tensorflow_probability as tfp\n","tfd = tfp.distributions\n","tf.config.experimental.enable_op_determinism()\n","tf.keras.backend.clear_session()"]},{"cell_type":"markdown","metadata":{"id":"aUvm9S0jssva"},"source":["### Set seeds and restart session to ensure reproducibility"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"crnG3mg6bU37"},"outputs":[],"source":["def reset_seeds_and_session(seed=1):\n","   os.environ[\"PYTHONHASHSEED\"]=str(seed)\n","   tf.random.set_seed(seed)\n","   np.random.seed(seed)\n","\n","   session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1,\n","                                           inter_op_parallelism_threads=1)\n","   sess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(),\n","                               config=session_conf)\n","   tf.compat.v1.keras.backend.set_session(sess)\n","\n","reset_seeds_and_session(seed=1)"]},{"cell_type":"markdown","metadata":{"id":"ad6lrbtPUwVk"},"source":["### Check TensorFlow and TensorFlow Probability versions"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1wemeM47_Dsy"},"outputs":[],"source":["f\"TF version: {tf.__version__}.\"\n","f\"TFP version: {tfp.__version__}.\""]},{"cell_type":"markdown","metadata":{"id":"rMuIKSLjUeBO"},"source":["### Check hardware"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2wcWtXa1UcSP"},"outputs":[],"source":["from tensorflow.python.profiler import profiler_client\n","\n","if tf.test.gpu_device_name():\n","    print(f\"Found GPU: {tf.test.gpu_device_name()}.\")\n","    !nvidia-smi\n","else:\n","    print(\"Found no GPU.\")\n","try:\n","    tpu_profile_service_address = os.environ[\"COLAB_TPU_ADDR\"].replace(\"8470\", \"8466\")\n","    print(f\"Found TPU: {profiler_client.monitor(tpu_profile_service_address, 100, 2)}.\")\n","except:\n","    print(\"Found no TPU.\")"]},{"cell_type":"markdown","metadata":{"id":"R1BPxWcLLwFp"},"source":["## Initial training"]},{"cell_type":"markdown","metadata":{"id":"-qQx0QfuLwFQ"},"source":["### Load custom modules"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SfLJzbL04VZ8"},"outputs":[],"source":["try:\n","    importlib.reload(classifier)\n","    importlib.reload(clfutils)\n","    print(\"Modules were reloaded.\")\n","except:\n","    import xpsdeeplearning.network.classifier as classifier\n","    import xpsdeeplearning.network.utils as clfutils\n","    print(\"Modules were loaded.\")"]},{"cell_type":"markdown","metadata":{"id":"j7M70DZczykg"},"source":["### Set up the parameters & folder structure\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GXy2WdXYXcXc"},"outputs":[],"source":["time = datetime.datetime.now().astimezone(pytz.timezone(\"Europe/Berlin\")).strftime(\"%Y%m%d_%Hh%Mm\")\n","exp_name = \"Ni_2_classes_linear_comb_regression_spatial_prior\"\n","\n","clf = classifier.Classifier(time=time,\n","                            exp_name=exp_name,\n","                            task=\"regression\",\n","                            intensity_only=True)\n","\n","### If labels not saved with data ###\n","# =============================================================================\n","# labels = [\"Fe metal\", \"FeO\", \"Fe3O4\", \"Fe2O3\"]\n","# clf = classifier.Classifier(time=time,\n","#                            exp_name=exp_name,\n","#                            task=\"regression\",\n","#                            intensity_only=True,\n","#                            labels=labels)\n","# ============================================================================="]},{"cell_type":"markdown","metadata":{"id":"q0wkRVOuLwFy"},"source":["### Load and inspect the data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KZIiyOyKtxmf"},"outputs":[],"source":["input_filepath = r\"/content/drive/My Drive/deepxps/datasets/20210528_Ni_linear_combination_small_gas_phase.h5\"\n","\n","train_test_split = 0.2\n","train_val_split = 0.2\n","no_of_examples = 100000\n","\n","X_train, X_val, X_test, y_train, y_val, y_test,\\\n","    sim_values_train, sim_values_val, sim_values_test =\\\n","        clf.load_data_preprocess(input_filepath=input_filepath,\n","                                 no_of_examples=no_of_examples,\n","                                 train_test_split=train_test_split,\n","                                 train_val_split=train_val_split)\n","\n","# Check how the examples are distributed across the classes.\n","class_distribution = clf.datahandler.check_class_distribution(clf.task)\n","clf.plot_class_distribution()\n","clf.plot_random(no_of_spectra=10, dataset=\"train\")"]},{"cell_type":"markdown","metadata":{"id":"mBXaNdCjvh36"},"source":["#### Other data"]},{"cell_type":"markdown","metadata":{"id":"pz1u9yX1CADi"},"source":["##### Only use classification data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UUuFaVy7GFes"},"outputs":[],"source":["### Only use classification data\n","if clf.task == \"classification\":\n","    X_train, X_val, X_test, y_train, y_val, y_test, \\\n","        sim_values_train, sim_values_val, sim_values_test =\\\n","            clf.datahandler._only_keep_classification_data()\n","\n","    clf.plot_random(no_of_spectra = 10, dataset = \"train\")\n","\n","elif clf.task == \"regression\":\n","    print(\"Dataset was not changed.\")"]},{"cell_type":"markdown","metadata":{"id":"SYtqpn7tZfa-"},"source":["##### MNIST"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FLDm3qpXafNh"},"outputs":[],"source":["### Loads MNIST dataset.###\n","import matplotlib.pyplot as plt\n","clf.datahandler.train_test_split = 0.1\n","clf.datahandler.train_val_split = 0.1\n","clf.datahandler.no_of_examples = 4000\n","\n","print(\"Loading MNIST dataset\")\n","(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()\n","\n","X = np.expand_dims(\n","    np.concatenate((X_train, X_test), axis=0),\n","    -1)[:no_of_examples]\n","X = X.astype(\"float32\") / 255\n","y = np.expand_dims(\n","    np.concatenate((y_train, y_test), axis=0),\n","    -1)[:no_of_examples]\n","clf.datahandler.X, clf.datahandler.y = X, y\n","\n","(\n","    X_train,\n","    X_val,\n","    X_test,\n","    y_train,\n","    y_val,\n","    y_test,\n"," ) = clf.datahandler._split_test_val_train(X, y)\n","\n","clf.datahandler.X_train, clf.datahandler.y_train = X_train, y_train\n","clf.datahandler.X_val, clf.datahandler.y_val = X_val, y_val\n","clf.datahandler.X_test, clf.datahandler.y_test = X_test, y_test\n","clf.datahandler.input_shape = (clf.datahandler.X_train.shape[1:])\n","clf.datahandler.num_classes = 10\n","clf.datahandler.labels = list(range(clf.datahandler.num_classes))\n","\n","print(\"X_train.shape =\", X_train.shape)\n","print(\"y_train.shape =\", y_train.shape)\n","print(\"X_val.shape =\", X_val.shape)\n","print(\"y_val.shape =\", y_val.shape)\n","print(\"X_test.shape =\", X_test.shape)\n","print(\"y_test.shape =\", y_test.shape)\n","\n","plt.imshow(X_train[0, :, :, 0], cmap=\"gist_gray\")\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"l8hcXqqXZl6U"},"source":["##### Shrunken babys"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mGiKb6SKMc_0"},"outputs":[],"source":["from PIL import Image\n","import glob\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","\n","# Loads shrunken baby dataset\n","clf.datahandler.train_test_split = 0.1\n","clf.datahandler.train_val_split = 0.1\n","clf.datahandler.no_of_examples = 836\n","num_classes = 10\n","\n","image_paths = glob.glob(r\"/content/drive/My Drive/deepxps/datasets/shrunken_baby_ds/*.png\")\n","\n","X = [np.array(Image.open(im)) for im in image_paths[:clf.datahandler.no_of_examples]]\n","X = [np.expand_dims(image, -1) for image in X] # add extra dimension to each image (126,126) --> (126,126,1)\n","X = np.array(X) # convert list of images to single array [(126,126,1)] --> (836, 126, 126, 1)\n","\n","# Read labels from file.\n","#labels = pd.read_csv(r\"/content/drive/My Drive/deepxps/datasets/shrunken_baby_ds/shrunken_baby_labels.csv\").to_numpy()\n","labels = pd.read_csv(r\"/content/drive/My Drive/deepxps/datasets/shrunken_baby_ds/shrunken_baby_labels.csv\").to_numpy()[:clf.datahandler.no_of_examples]\n","#y = pd.read_csv(r\"/content/drive/My Drive/deepxps/datasets/shrunken_baby_ds/shrunken_baby_labels_new.csv\").to_numpy()[:clf.datahandler.no_of_examples]\n","\n","# Get labels from data\n","child_positions = [np.where(X[i] > 0.2) for i in range(clf.datahandler.no_of_examples)]\n","y = np.array([np.max(child_positions[i][0]) - np.min(child_positions[i][0]) for i in range(clf.datahandler.no_of_examples)])\n","y = np.expand_dims(y, -1)\n","\n","# Shrunk data by half.\n","from skimage.measure import block_reduce\n","#X = X[:,5:-21,13:-13,:]\n","X_new = []\n","for image in X:\n","    reduced_image = block_reduce(image,\n","                                 block_size=(2, 2, 1),\n","                                 func=np.mean)\n","    X_new.append(reduced_image)\n","X_new = np.array(X_new)\n","child_positions_shrunk = [np.where(X_new[i] > 0.2) for i in range(clf.datahandler.no_of_examples)]\n","y_shrunk = np.array([np.max(child_positions_shrunk[i][0]) - np.min(child_positions_shrunk[i][0]) for i in range(clf.datahandler.no_of_examples)])\n","y_shrunk = np.expand_dims(y_shrunk, -1)\n","\n","# Normalize data\n","X = X.astype(\"float32\") / 255\n","X_new = X_new.astype(\"float32\") / 255\n","\n","# Plot original and shrunken data\n","for i in range(5):\n","    r = np.random.randint(0,X.shape[0])\n","    fig, ax = plt.subplots(nrows=1, ncols=2)\n","\n","    child_position = np.where(X[r] > 0.2)\n","    min_height, max_height = np.min(child_position[0]), np.max(child_position[0])\n","    min_height_hor = child_position[1][np.argmin(child_position[0])]\n","    max_height_hor = child_position[1][np.argmax(child_position[0])]\n","    child_position_new = np.where(X_new[r] > 50/255.0)\n","    min_height_new, max_height_new = np.min(child_position_new[0]), np.max(child_position_new[0])\n","    min_height_hor_new = child_position_new[1][np.argmin(child_position_new[0])]\n","    max_height_hor_new = child_position_new[1][np.argmax(child_position_new[0])]\n","    real_size = int(y[r])\n","    shrunk_size = max_height_new - min_height_new\n","    print(f\"Child no. {r}, real size: {real_size}, new size: {shrunk_size}, factor: {np.round(real_size/shrunk_size,2)}\")\n","\n","    _ = ax[0].imshow(np.squeeze(X[r]))\n","    _ = ax[0].scatter(min_height_hor, min_height,  s=50, c=\"red\", marker=\".\")\n","    _ = ax[0].scatter(max_height_hor, max_height,  s=50, c=\"blue\", marker=\".\")\n","    _ = ax[1].imshow(np.squeeze(X_new[r]))\n","    _ = ax[1].scatter(min_height_hor_new, min_height_new,  s=50, c=\"red\", marker=\".\")\n","    _ = ax[1].scatter(max_height_hor_new, max_height_new,  s=50, c=\"blue\", marker=\".\")\n","    plt.show()\n","\n","# Store data in clf.datahandler object\n","clf.datahandler.X, clf.datahandler.y = X_new, y_shrunk\n","\n","(\n","    clf.datahandler.X_train,\n","    clf.datahandler.X_val,\n","    clf.datahandler.X_test,\n","    clf.datahandler.y_train,\n","    clf.datahandler.y_val,\n","    clf.datahandler.y_test,\n"," ) = clf.datahandler._split_test_val_train(\n","     clf.datahandler.X,\n","     clf.datahandler.y)\n","\n","clf.datahandler.input_shape = (clf.datahandler.X_train.shape[1:])\n","clf.datahandler.num_classes = 1\n","clf.datahandler.labels = [\"sizes\"]\n","\n","print(\"X.shape =\", clf.datahandler.X.shape)\n","print(\"y.shape =\", clf.datahandler.y.shape)\n","print(\"X_train.shape =\", clf.datahandler.X_train.shape)\n","print(\"y_train.shape =\", clf.datahandler.y_train.shape)\n","print(\"X_val.shape =\", clf.datahandler.X_val.shape)\n","print(\"y_val.shape =\", clf.datahandler.y_val.shape)\n","print(\"X_test.shape =\", clf.datahandler.X_test.shape)\n","print(\"y_test.shape =\", clf.datahandler.y_test.shape)\n","print(\"\\n\")"]},{"cell_type":"markdown","metadata":{"id":"tq0d4P6_Zhss"},"source":["##### Housing prices"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XLk7Oqm_baL7"},"outputs":[],"source":["clf.datahandler.train_test_split = 0.1\n","clf.datahandler.train_val_split = 0.1\n","clf.datahandler.no_of_examples = 100\n","\n","from keras.datasets import boston_housing\n","(X_train, y_train), (X_test, y_test) = boston_housing.load_data()\n","\n","X = np.expand_dims(\n","    np.concatenate((X_train, X_test), axis=0),\n","    -1)[:no_of_examples]\n","y = np.expand_dims(\n","    np.concatenate((y_train, y_test), axis=0),\n","    -1)[:no_of_examples]\n","clf.datahandler.X, clf.datahandler.y = X, y\n","\n","(\n","    X_train,\n","    X_val,\n","    X_test,\n","    y_train,\n","    y_val,\n","    y_test,\n"," ) = clf.datahandler._split_test_val_train(X, y)\n","\n","\n","clf.datahandler.X_train, clf.datahandler.y_train = X_train, y_train\n","clf.datahandler.X_val, clf.datahandler.y_val = X_val, y_val\n","clf.datahandler.X_test, clf.datahandler.y_test = X_test, y_test\n","clf.datahandler.input_shape = (clf.datahandler.X_train.shape[1:])\n","clf.datahandler.num_classes = 1\n","clf.datahandler.labels = [\"prizes\"]\n","\n","print(\"X_train.shape =\", X_train.shape)\n","print(\"y_train.shape =\", y_train.shape)\n","print(\"X_val.shape =\", X_val.shape)\n","print(\"y_val.shape =\", y_val.shape)\n","print(\"X_test.shape =\", X_test.shape)\n","print(\"y_test.shape =\", y_test.shape)"]},{"cell_type":"markdown","metadata":{"id":"TKXmi1LMLwF6"},"source":["### Design the model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pyxJ9_qh5awz"},"outputs":[],"source":["try:\n","    importlib.reload(models)\n","    print(\"Models module was reloaded.\")\n","except:\n","    import xpsdeeplearning.network.models as models\n","    print(\"Models module was loaded.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2xJuwB18HdT3"},"outputs":[],"source":["    def __init__(self, shape, loc, scale, *, lengthscale=1.0):\n","        \"\"\"Samples `.p` from a correlated Gaussian of dimension\n","        `shape[-2]*shape[-1]`\"\"\"\n","        loc = torch.tensor(loc).unsqueeze(0)\n","\n","\n","        super().__init__(shape, loc=loc, scale=scale, distance_matrix=d,\n","                         lengthscale=lengthscale)\n","\n","    # we have to move the kernel evaluation into the _dist, so that the hierarchical versions of this prior work\n","    # for fixed parameters, we could consider precomputing it to save a bit of time here\n","    _dist = SquaredExponentialNormal\n","\n","class SquaredExponentialNormal(td.MultivariateNormal):\n","    \"\"\"Multivariate Normal with a squared exponential kernel as covariance\"\"\"\n","    def __init__(self, loc, scale, distance_matrix, lengthscale):\n","\n","\n","def prior_fn(loc):\n","    lengthscale = 1.0\n","    shape =\n","\n","    # generates all the points in a grid from (0,0) to (shape[-2], shape[-1])\n","    p = np.mgrid[:shape[-2], :shape[-1]].reshape(2, -1).T\n","    # computes the matrix of Euclidean distances between all the points in p\n","    distance_matrix = np.sum((p[:, None, :] - p[None, :, :]) ** 2.0, 2) ** 0.5\n","    cov = tf.math.exp.exp(- distance_matrix / lengthscale) * scale ** 2.0\n","    #cov = torch.cholesky(cov)\n","\n","    return tfp.distributions.MultivariateNormalFullCovariance(\n","         loc=loc,\n","         covariance_matrix=cov)\n","\n","\n","class SpatialBayesianCNN(models.EmptyModel):\n","    \"\"\"\n","    A CNN with three convolutional layers of different kernel size at\n","    the beginning. Works well for learning across scales.\n","\n","    \"\"\"\n","    def __init__(\n","        self,\n","        inputshape,\n","        num_classes,\n","        kernel_prior_fn,\n","        kl_divergence_fn,\n","        task,\n","        ):\n","        if len(inputshape) == 2:\n","            conv_layer = tfp.layers.Convolution1DFlipout\n","            strides = 1\n","            average_pool_layer = layers.AveragePooling1D\n","        elif len(inputshape) == 3:\n","            conv_layer = tfp.layers.Convolution2DFlipout\n","            strides = (1,1)\n","            average_pool_layer =  layers.AveragePooling2D\n","\n","        if (task == \"regression\" or task == \"multi_class_detection\"):\n","            if num_classes == 1:\n","                output_act = None\n","            else:\n","                output_act = \"sigmoid\"\n","\n","        elif task == \"classification\":\n","            output_act = \"softmax\"\n","\n","        ## Change activation in Bayesian layers?\n","        prob_act = \"relu\"\n","\n","        self.input_1 = layers.Input(\n","            shape = inputshape,\n","            name=\"input_1\")\n","        self.conv_1_short = conv_layer(\n","            filters=12,\n","            kernel_size=5,\n","            strides=strides,\n","            padding=\"same\",\n","            kernel_prior_fn=kernel_prior_fn,\n","            kernel_divergence_fn=kl_divergence_fn,\n","            bias_divergence_fn=kl_divergence_fn,\n","            activation=prob_act,\n","            name=\"conv_1_short\")(self.input_1)\n","        self.conv_1_medium = conv_layer(\n","            filters=12,\n","            kernel_size=10,\n","            strides=strides,\n","            padding=\"same\",\n","            kernel_prior_fn=kernel_prior_fn,\n","            kernel_divergence_fn=kl_divergence_fn,\n","            bias_divergence_fn=kl_divergence_fn,\n","            activation=prob_act,\n","            name=\"conv_1_medium\")(self.input_1)\n","        self.conv_1_long = conv_layer(\n","            filters=12,\n","            kernel_size=15,\n","            strides=strides,\n","            padding=\"same\",\n","            kernel_prior_fn=kernel_prior_fn,\n","            kernel_divergence_fn=kl_divergence_fn,\n","            bias_divergence_fn=kl_divergence_fn,\n","            activation=prob_act,\n","            name=\"conv_1_long\")(self.input_1)\n","\n","        sublayers = [self.conv_1_short, self.conv_1_medium, self.conv_1_long]\n","        merged_sublayers = layers.concatenate(sublayers)\n","\n","        self.conv_2 = conv_layer(\n","            filters=10,\n","            kernel_size=5,\n","            strides=strides,\n","            padding=\"valid\",\n","            kernel_prior_fn=kernel_prior_fn,\n","            kernel_divergence_fn=kl_divergence_fn,\n","            bias_divergence_fn=kl_divergence_fn,\n","            activation=prob_act,\n","            name=\"conv_2\")(merged_sublayers)\n","        self.conv_3 = conv_layer(\n","            filters=10,\n","            kernel_size=5,\n","            strides=strides,\n","            padding=\"valid\",\n","            kernel_prior_fn=kernel_prior_fn,\n","            kernel_divergence_fn=kl_divergence_fn,\n","            bias_divergence_fn=kl_divergence_fn,\n","            activation=prob_act,\n","            name=\"conv_3\")(self.conv_2)\n","        self.average_pool_1 = average_pool_layer(\n","            name=\"average_pool_1\")(self.conv_3)\n","\n","        self.flatten_1 = layers.Flatten(name=\"flatten1\")(self.average_pool_1)\n","        self.drop_1 = layers.Dropout(\n","            rate=0.2,\n","            name=\"drop_1\")(self.flatten_1)\n","        self.dense_1 = tfp.layers.DenseFlipout(\n","            units=4000,\n","            kernel_divergence_fn=kl_divergence_fn,\n","            bias_divergence_fn=kl_divergence_fn,\n","            activation=prob_act,\n","            name=\"dense_1\")(self.flatten_1)\n","\n","        self.dense_2 = tfp.layers.DenseFlipout(\n","            units=num_classes,\n","            kernel_divergence_fn=kl_divergence_fn,\n","            bias_divergence_fn=kl_divergence_fn,\n","            activation=output_act,\n","            name=\"dense_2\")(self.dense_1)\n","\n","        self.outputs = self.dense_2\n","        # self.outputs = tfp.layers.IndependentNormal(event_shape=num_classes)(self.dense_2)\n","\n","        no_of_inputs = len(sublayers)\n","\n","        super(SpatialBayesianCNN, self).__init__(\n","            inputs=self.input_1,\n","            outputs=self.outputs,\n","            inputshape=inputshape,\n","            num_classes=num_classes,\n","            no_of_inputs=no_of_inputs,\n","            name=\"SpatialBayesianCNN\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xT2XmoLCbXoM"},"outputs":[],"source":["class BayesianCNN(models.EmptyModel):\n","    \"\"\"\n","    A CNN with three convolutional layers of different kernel size at\n","    the beginning. Works well for learning across scales.\n","\n","    \"\"\"\n","    def __init__(\n","        self,\n","        inputshape,\n","        num_classes,\n","        kl_divergence_fn,\n","        task,\n","        ):\n","        if len(inputshape) == 2:\n","            conv_layer = tfp.layers.Convolution1DFlipout\n","            strides = 1\n","            average_pool_layer = layers.AveragePooling1D\n","        elif len(inputshape) == 3:\n","            conv_layer = tfp.layers.Convolution2DFlipout\n","            strides = (1,1)\n","            average_pool_layer =  layers.AveragePooling2D\n","\n","        if (task == \"regression\" or task == \"multi_class_detection\"):\n","            if num_classes == 1:\n","                output_act = None\n","            else:\n","                output_act = \"sigmoid\"\n","\n","        elif task == \"classification\":\n","            output_act = \"softmax\"\n","\n","        ## Change activation in Bayesian layers?\n","        prob_act = \"relu\"\n","\n","        self.input_1 = layers.Input(\n","            shape = inputshape,\n","            name=\"input_1\")\n","        self.conv_1_short = conv_layer(\n","            filters=12,\n","            kernel_size=5,\n","            strides=strides,\n","            padding=\"same\",\n","            kernel_divergence_fn=kl_divergence_fn,\n","            bias_divergence_fn=kl_divergence_fn,\n","            activation=prob_act,\n","            name=\"conv_1_short\")(self.input_1)\n","        self.conv_1_medium = conv_layer(\n","            filters=12,\n","            kernel_size=10,\n","            strides=strides,\n","            padding=\"same\",\n","            kernel_divergence_fn=kl_divergence_fn,\n","            bias_divergence_fn=kl_divergence_fn,\n","            activation=prob_act,\n","            name=\"conv_1_medium\")(self.input_1)\n","        self.conv_1_long = conv_layer(\n","            filters=12,\n","            kernel_size=15,\n","            strides=strides,\n","            padding=\"same\",\n","            kernel_divergence_fn=kl_divergence_fn,\n","            bias_divergence_fn=kl_divergence_fn,\n","            activation=prob_act,\n","            name=\"conv_1_long\")(self.input_1)\n","\n","        sublayers = [self.conv_1_short, self.conv_1_medium, self.conv_1_long]\n","        merged_sublayers = layers.concatenate(sublayers)\n","\n","        self.conv_2 = conv_layer(\n","            filters=10,\n","            kernel_size=5,\n","            strides=strides,\n","            padding=\"valid\",\n","            kernel_divergence_fn=kl_divergence_fn,\n","            bias_divergence_fn=kl_divergence_fn,\n","            activation=prob_act,\n","            name=\"conv_2\")(merged_sublayers)\n","        self.conv_3 = conv_layer(\n","            filters=10,\n","            kernel_size=5,\n","            strides=strides,\n","            padding=\"valid\",\n","            kernel_divergence_fn=kl_divergence_fn,\n","            bias_divergence_fn=kl_divergence_fn,\n","            activation=prob_act,\n","            name=\"conv_3\")(self.conv_2)\n","        self.average_pool_1 = average_pool_layer(\n","            name=\"average_pool_1\")(self.conv_3)\n","\n","        self.flatten_1 = layers.Flatten(name=\"flatten1\")(self.average_pool_1)\n","        self.drop_1 = layers.Dropout(\n","            rate=0.2,\n","            name=\"drop_1\")(self.flatten_1)\n","        self.dense_1 = tfp.layers.DenseFlipout(\n","            units=4000,\n","            kernel_divergence_fn=kl_divergence_fn,\n","            bias_divergence_fn=kl_divergence_fn,\n","            activation=prob_act,\n","            name=\"dense_1\")(self.flatten_1)\n","\n","        self.dense_2 = tfp.layers.DenseFlipout(\n","            units=num_classes,\n","            kernel_divergence_fn=kl_divergence_fn,\n","            bias_divergence_fn=kl_divergence_fn,\n","            activation=output_act,\n","            name=\"dense_2\")(self.dense_1)\n","\n","        self.outputs = self.dense_2\n","        # self.outputs = tfp.layers.IndependentNormal(event_shape=num_classes)(self.dense_2)\n","\n","# =============================================================================\n","#         s0, s1 = tf.split(self.dense_2, num_classes)\n","#         self.outputs = tfp.layers.DistributionLambda(lambda t: tfd.Normal(t[0], t[1]))(s0, s1)\n","#         self.dense_2 = tfp.layers.DenseFlipout(\n","#             units=tfp.layers.IndependentNormal.params_size(num_classes),\n","#             activation=output_act,\n","#             kernel_divergence_fn=kl_divergence_fn,\n","#             bias_divergence_fn=kl_divergence_fn)(self.dense_1)\n","# =============================================================================\n","# =============================================================================\n","#         self.dense_2 = layers.Dense(\n","#             units=num_classes+num_classes,\n","#             activation=output_act,\n","#             name=\"dense_2\")(self.dense_1)\n","#         self.output = tfp.layers.DistributionLambda(\n","#             lambda t: tfd.LogNormal(loc=t[..., :1], scale=tf.math.softplus(0.05 * t[..., 1:]))),])\n","# =============================================================================\n","\n","        no_of_inputs = len(sublayers)\n","\n","        super(BayesianCNN, self).__init__(\n","            inputs=self.input_1,\n","            outputs=self.outputs,\n","            inputshape=inputshape,\n","            num_classes=num_classes,\n","            no_of_inputs=no_of_inputs,\n","            name=\"BayesianCNN\")\n","\n","class LastLayerBayesianCNN(models.EmptyModel):\n","    \"\"\"\n","    A CNN with three convolutional layers of different kernel size at\n","    the beginning. Works well for learning across scales.\n","\n","    \"\"\"\n","    def __init__(\n","        self,\n","        inputshape,\n","        num_classes,\n","        kl_divergence_fn,\n","        task,\n","        ):\n","        if len(inputshape) == 2:\n","            conv_layer = tfp.layers.Convolution1DFlipout\n","            strides = 1\n","            average_pool_layer = layers.AveragePooling1D\n","        elif len(inputshape) == 3:\n","            conv_layer = tfp.layers.Convolution2DFlipout\n","            strides = (1,1)\n","            average_pool_layer =  layers.AveragePooling2D\n","\n","        if (task == \"regression\" or task == \"multi_class_detection\"):\n","            if num_classes == 1:\n","                output_act = None\n","            else:\n","                output_act = \"sigmoid\"\n","\n","        elif task == \"classification\":\n","            output_act = \"softmax\"\n","\n","        ## Change activation in Bayesian layers?\n","        prob_act = \"relu\"\n","\n","        self.input_1 = layers.Input(\n","            shape = inputshape,\n","            name=\"input_1\")\n","        self.conv_1_short = layers.Conv1D(\n","            filters=12,\n","            kernel_size=5,\n","            strides=strides,\n","            padding=\"same\",\n","            activation=\"relu\",\n","            name=\"conv_1_short\")(self.input_1)\n","        self.conv_1_medium = layers.Conv1D(\n","            filters=12,\n","            kernel_size=10,\n","            strides=strides,\n","            padding=\"same\",\n","            activation=\"relu\",\n","            name=\"conv_1_medium\")(self.input_1)\n","        self.conv_1_long = layers.Conv1D(\n","            filters=12,\n","            kernel_size=15,\n","            strides=strides,\n","            padding=\"same\",\n","            activation=\"relu\",\n","            name=\"conv_1_long\")(self.input_1)\n","\n","        sublayers = [self.conv_1_short, self.conv_1_medium, self.conv_1_long]\n","        merged_sublayers = layers.concatenate(sublayers)\n","\n","        self.conv_2 = layers.Conv1D(\n","            filters=10,\n","            kernel_size=5,\n","            strides=strides,\n","            padding=\"valid\",\n","            activation=\"relu\",\n","            name=\"conv_2\")(merged_sublayers)\n","        self.conv_3 = layers.Conv1D(\n","            filters=10,\n","            kernel_size=5,\n","            strides=strides,\n","            padding=\"valid\",\n","            activation=\"relu\",\n","            name=\"conv_3\")(self.conv_2)\n","        self.average_pool_1 = average_pool_layer(\n","            name=\"average_pool_1\")(self.conv_3)\n","        self.flatten_1 = layers.Flatten(\n","            name=\"flatten1\")(self.average_pool_1)\n","        self.drop_1 = layers.Dropout(\n","            rate=0.2,\n","            name=\"drop_1\")(self.flatten_1)\n","        self.dense_1 = layers.Dense(\n","            units=4000,\n","            activation=\"relu\",\n","            name=\"dense_1\")(self.flatten_1)\n","\n","        self.dense_2 = tfp.layers.DenseFlipout(\n","            units=num_classes,\n","            kernel_divergence_fn=kl_divergence_fn,\n","            bias_divergence_fn=kl_divergence_fn,\n","            activation=output_act,\n","            name=\"dense_2\")(self.dense_1)\n","\n","        self.outputs = self.dense_2\n","\n","        no_of_inputs = len(sublayers)\n","\n","        super(LastLayerBayesianCNN, self).__init__(\n","            inputs=self.input_1,\n","            outputs=self.outputs,\n","            inputshape=inputshape,\n","            num_classes=num_classes,\n","            no_of_inputs=no_of_inputs,\n","            name=\"LastLayerBayesianCNN\")"]},{"cell_type":"markdown","metadata":{"id":"0FDlB_v8VaAR"},"source":["#### Not used"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"d1Wlq08LYABx"},"outputs":[],"source":["from tensorflow.keras import layers\n","from tensorflow.keras.initializers import glorot_uniform\n","from tensorflow.python.keras import backend as K\n","\n","class CNN(models.EmptyModel):\n","    \"\"\"\n","    A CNN with three convolutional layers of different kernel size at\n","    the beginning. Works well for learning across scales.\n","\n","    \"\"\"\n","    def __init__(\n","        self,\n","        inputshape,\n","        num_classes,\n","        task,\n","        ):\n","        if len(inputshape) == 2:\n","            conv_layer = layers.Conv1D\n","            strides = 1\n","            average_pool_layer = layers.AveragePooling1D\n","        elif len(inputshape) == 3:\n","            conv_layer = layers.Conv2D\n","            strides = (1,1)\n","            average_pool_layer =  layers.AveragePooling2D\n","\n","        if (task == \"regression\" or task == \"multi_class_detection\"):\n","            if num_classes == 1:\n","                output_act = None\n","            else:\n","                output_act = \"sigmoid\"\n","\n","        elif task == \"classification\":\n","            output_act = \"softmax\"\n","\n","        self.input_1 = layers.Input(\n","            shape = inputshape,\n","            name=\"input_1\")\n","        self.conv_1_short = conv_layer(\n","            filters=12,\n","            kernel_size=5,\n","            strides=strides,\n","            padding=\"same\",\n","            activation=\"relu\",\n","            name=\"conv_1_short\")(self.input_1)\n","        self.conv_1_medium = conv_layer(\n","            filters=12,\n","            kernel_size=10,\n","            strides=strides,\n","            padding=\"same\",\n","            activation=\"relu\",\n","            name=\"conv_1_medium\")(self.input_1)\n","        self.conv_1_long = conv_layer(\n","            filters=12,\n","            kernel_size=15,\n","            strides=strides,\n","            padding=\"same\",\n","            activation=\"relu\",\n","            name=\"conv_1_long\")(self.input_1)\n","\n","        sublayers = [self.conv_1_short, self.conv_1_medium, self.conv_1_long]\n","        merged_sublayers = layers.concatenate(sublayers)\n","\n","        self.conv_2 = conv_layer(\n","            filters=10,\n","            kernel_size=5,\n","            strides=strides,\n","            padding=\"valid\",\n","            activation=\"relu\",\n","            name=\"conv_2\")(merged_sublayers)\n","        self.conv_3 = conv_layer(\n","            filters=10,\n","            kernel_size=5,\n","            strides=strides,\n","            padding=\"valid\",\n","            activation=\"relu\",\n","            name=\"conv_3\")(self.conv_2)\n","        self.average_pool_1 = average_pool_layer(\n","            name=\"average_pool_1\")(self.conv_3)\n","        self.flatten_1 = layers.Flatten(\n","            name=\"flatten1\")(self.average_pool_1)\n","        self.drop_1 = layers.Dropout(\n","            rate=0.2,\n","            name=\"drop_1\")(self.flatten_1)\n","        self.dense_1 = layers.Dense(\n","            units=4000,\n","            activation=\"relu\",\n","            name=\"dense_1\")(self.flatten_1)\n","        self.dense_2 = layers.Dense(\n","            units=num_classes,\n","            activation=output_act,\n","            name=\"dense_2\")(self.dense_1)\n","\n","        if task == \"regression\":\n","            self.outputs = layers.Lambda(\n","                lambda x: x/tf.reshape(K.sum(x, axis=-1),(-1,1)),\n","                name = \"output_normalization\")(self.dense_2)\n","\n","        else:\n","            self.outputs = self.dense_2\n","\n","        no_of_inputs = len(sublayers)\n","\n","        super(CNN, self).__init__(\n","            inputs=self.input_1,\n","            outputs=self.outputs,\n","            inputshape=inputshape,\n","            num_classes=num_classes,\n","            no_of_inputs=no_of_inputs,\n","            name=\"CNN\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Y9cO1uT4iZnP"},"outputs":[],"source":["from tensorflow.keras import layers\n","from tensorflow.keras.initializers import glorot_uniform\n","from tensorflow.python.keras import backend as K\n","\n","class DropoutCNN(models.EmptyModel):\n","    \"\"\"\n","    A CNN with three convolutional layers of different kernel size at\n","    the beginning. Works well for learning across scales.\n","\n","    \"\"\"\n","    def __init__(\n","        self,\n","        inputshape,\n","        num_classes,\n","        task,\n","        ):\n","        drop_rate = 0.3 # change dropout rate here\n","\n","        if len(inputshape) == 2:\n","            conv_layer = layers.Conv1D\n","            strides = 1\n","            average_pool_layer = layers.AveragePooling1D\n","        elif len(inputshape) == 3:\n","            conv_layer = layers.Conv2D\n","            strides = (1,1)\n","            average_pool_layer =  layers.AveragePooling2D\n","\n","        if (task == \"regression\" or task == \"multi_class_detection\"):\n","            if num_classes == 1:\n","                output_act = None\n","            else:\n","                output_act = \"sigmoid\"\n","\n","        elif task == \"classification\":\n","            output_act = \"softmax\"\n","\n","        self.input_1 = layers.Input(\n","            shape = inputshape,\n","            name=\"input_1\")\n","        self.conv_1_short = conv_layer(\n","            filters=12,\n","            kernel_size=5,\n","            strides=strides,\n","            padding=\"same\",\n","            activation=\"relu\",\n","            name=\"conv_1_short\")(self.input_1)\n","        self.conv_1_medium = conv_layer(\n","            filters=12,\n","            kernel_size=10,\n","            strides=strides,\n","            padding=\"same\",\n","            activation=\"relu\",\n","            name=\"conv_1_medium\")(self.input_1)\n","        self.conv_1_long = conv_layer(\n","            filters=12,\n","            kernel_size=15,\n","            strides=strides,\n","            padding=\"same\",\n","            activation=\"relu\",\n","            name=\"conv_1_long\")(self.input_1)\n","\n","        sublayers = [self.conv_1_short, self.conv_1_medium, self.conv_1_long]\n","        merged_sublayers = layers.concatenate(sublayers, name=\"conv_concat\")\n","\n","        self.conv_1_drop = layers.Dropout(\n","            rate=drop_rate,\n","            name=\"conv_1_drop\")(merged_sublayers, training=True)\n","        self.conv_2 = conv_layer(\n","            filters=10,\n","            kernel_size=5,\n","            strides=strides,\n","            padding=\"valid\",\n","            activation=\"relu\",\n","            name=\"conv_2\")(self.conv_1_drop)\n","        self.conv_2_drop = layers.Dropout(\n","            rate=drop_rate,\n","            name=\"conv_2_drop\")(self.conv_2, training=True)\n","        self.conv_3 = conv_layer(\n","            filters=10,\n","            kernel_size=5,\n","            strides=strides,\n","            padding=\"valid\",\n","            activation=\"relu\",\n","            name=\"conv_3\")(self.conv_2_drop)\n","        self.conv_3_drop = layers.Dropout(\n","            rate=drop_rate,\n","            name=\"conv_3_drop\")(self.conv_3, training=True)\n","        self.average_pool_1 = average_pool_layer(\n","            name=\"average_pool_1\")(self.conv_3_drop)\n","        self.flatten_1 = layers.Flatten(\n","            name=\"flatten_1\")(self.average_pool_1)\n","        self.dense_1 = layers.Dense(\n","            units=4000,\n","            activation=\"relu\",\n","            name=\"dense_1\")(self.flatten_1)\n","        self.dense_2 = layers.Dense(\n","            units=num_classes,\n","            activation=output_act,\n","            name=\"dense_2\")(self.dense_1)\n","\n","        if task == \"regression\":\n","            self.outputs = layers.Lambda(\n","                lambda x: x/tf.reshape(K.sum(x, axis=-1),(-1,1)),\n","                name = \"output_normalization\")(self.dense_2)\n","\n","        else:\n","            self.outputs = self.dense_2\n","\n","        no_of_inputs = len(sublayers)\n","\n","        super(DropoutCNN, self).__init__(\n","            inputs=self.input_1,\n","            outputs=self.outputs,\n","            inputshape=inputshape,\n","            num_classes=num_classes,\n","            no_of_inputs=no_of_inputs,\n","            name=\"DropoutCNN\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9X0ydiIuf4sn"},"outputs":[],"source":["from tensorflow.keras import layers\n","from tensorflow.keras.initializers import glorot_uniform\n","from tensorflow.python.keras import backend as K\n","\n","class CNN2(models.EmptyModel):\n","    def __init__(\n","        self,\n","        inputshape,\n","        num_classes,\n","        task,\n","        ):\n","        \"\"\"\n","        https://github.com/DoctorLoop/BayesianDeepLearning/blob/master/Chapter3_TensorFlowProbability_BayesianConvNets.ipynb\n","        \"\"\"\n","\n","        if len(inputshape) == 2:\n","            conv_layer = layers.Conv1D\n","            max_pool_layer = layers.MaxPool1D\n","            pool_size=4\n","\n","        elif len(inputshape) == 3:\n","            conv_layer = layers.Conv2D\n","            max_pool_layer =  layers.MaxPool2D\n","            pool_size=(4,4)\n","\n","        if task == \"regression\":\n","            if num_classes == 1:\n","                output_act = None\n","            else:\n","                output_act = \"sigmoid\"\n","        elif task == \"classification\":\n","            output_act = \"softmax\"\n","\n","        self.input_1 = layers.Input(\n","            shape = inputshape,\n","            name=\"input_1\")\n","        self.conv_1 = conv_layer(\n","            filters=16,\n","            kernel_size=5,\n","            activation = \"relu\",\n","            name=\"conv_1\")(self.input_1)\n","        self.max_pool_1 = max_pool_layer(\n","            pool_size=pool_size\n","        )(self.conv_1)\n","        self.conv_2 = conv_layer(\n","            filters=32,\n","            kernel_size=3,\n","            activation = \"relu\",\n","            name=\"conv_2\")(self.max_pool_1)\n","        self.max_pool_2 = max_pool_layer(\n","            pool_size=pool_size\n","        )(self.conv_2)\n","        self.flatten_1 = layers.Flatten(\n","            name=\"flatten_1\"\n","        )(self.max_pool_2),\n","        self.dense_1 = tfp.layers.DenseFlipout(\n","            units=num_classes,\n","            activation = output_act,\n","            name=\"dense_1\")(self.flatten_1)\n","\n","        no_of_inputs = 1\n","\n","        super(CNN2, self).__init__(\n","            inputs=self.input_1,\n","            outputs=self.dense_1,\n","            inputshape=inputshape,\n","            num_classes=num_classes,\n","            no_of_inputs=no_of_inputs,\n","            name=\"CNN2\")\n","\n","class BayesianCNN2(models.EmptyModel):\n","    def __init__(\n","        self,\n","        inputshape,\n","        num_classes,\n","        kl_divergence_fn,\n","        task,\n","        ):\n","\n","        if len(inputshape) == 2:\n","            conv_layer = tfp.layers.Convolution1DFlipout\n","            max_pool_layer = layers.MaxPool1D\n","            pool_size=4\n","\n","        elif len(inputshape) == 3:\n","            conv_layer = tfp.layers.Convolution2DFlipout\n","            max_pool_layer =  layers.MaxPool2D\n","            pool_size=(4,4)\n","\n","        if (task == \"regression\" or task == \"multi_class_detection\"):\n","            if num_classes == 1:\n","                output_act = None\n","            else:\n","                output_act = \"sigmoid\"\n","        elif task == \"classification\":\n","            output_act = \"softmax\"\n","\n","        ## Change activation in Bayesian layers?\n","        prob_act = \"relu\"\n","\n","        self.input_1 = layers.Input(\n","            shape = inputshape,\n","            name=\"input_1\")\n","        self.conv_1 = conv_layer(\n","            filters=16,\n","            kernel_size=5,\n","            activation = prob_act,\n","            kernel_divergence_fn=kernel_divergence_fn,\n","            bias_divergence_fn=kl_divergence_function,\n","            name=\"conv_1\")(self.input_1)\n","        self.max_pool_1 = max_pool_layer(\n","            pool_size=pool_size\n","        )(self.conv_1)\n","        self.conv_2 = conv_layer(\n","            filters=32,\n","            kernel_size=3,\n","            activation = prob_act,\n","            kernel_divergence_fn=kl_divergence_fn,\n","            bias_divergence_fn=kl_divergence_function,\n","            name=\"conv_2\")(self.max_pool_1)\n","        self.max_pool_2 = max_pool_layer(\n","            pool_size=pool_size\n","        )(self.conv_2)\n","        self.flatten_1 = layers.Flatten(\n","            name=\"flatten_1\"\n","        )(self.max_pool_2),\n","        self.dense_1 = tfp.layers.DenseFlipout(\n","            units=num_classes,\n","            activation = output_act,\n","            kernel_divergence_fn=kl_divergence_fn,\n","            bias_divergence_fn=kl_divergence_function,\n","            name=\"dense_1\")(self.flatten_1)\n","\n","        no_of_inputs = 1\n","\n","        super(BayesianCNN2, self).__init__(\n","            inputs=self.input_1,\n","            outputs=self.dense_1,\n","            inputshape=inputshape,\n","            num_classes=num_classes,\n","            no_of_inputs=no_of_inputs,\n","            name=\"BayesianCNN2\")"]},{"cell_type":"markdown","metadata":{"id":"76OKygMa9Xr9"},"source":["#### Design Kullback-Leibler divergence function"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XVts9GTPXueH"},"outputs":[],"source":["kl_divergence_fn = (\n","    lambda q, p, _: (tfp.distributions.kl_divergence(q, p)) /\n","    tf.cast(clf.datahandler.X_train.shape[0], dtype=tf.float32)\n","    )"]},{"cell_type":"markdown","metadata":{"id":"zHpFBDmgXoMW"},"source":["#### Build the model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Af3wbjEW9Xr-"},"outputs":[],"source":["#clf.model = CNN(\n","#    inputshape=clf.datahandler.input_shape,\n","#    num_classes=clf.datahandler.num_classes,\n","#    task=clf.task)\n","\n","clf.model = DropoutCNN(\n","    inputshape=clf.datahandler.input_shape,\n","    num_classes=clf.datahandler.num_classes,\n","    task=clf.task)\n","\n","#clf.model = BayesianCNN(\n","#    inputshape=clf.datahandler.input_shape,\n","#    num_classes=clf.datahandler.num_classes,\n","#    kl_divergence_fn=kl_divergence_fn,\n","#    task=clf.task)\n","\n","#clf.model = LastLayerBayesianCNN(\n","#    inputshape=clf.datahandler.input_shape,\n","#    num_classes=clf.datahandler.num_classes,\n","#    kl_divergence_fn=kl_divergence_fn,\n","#    task=clf.task)\n","\n","#clf.model = CNN2(\n","#    inputshape=clf.datahandler.input_shape,\n","#    num_classes=clf.datahandler.num_classes,\n","#    task=clf.task)\n","\n","# clf.model = BayesianCNN2(\n","#     inputshape=clf.datahandler.input_shape,\n","#     num_classes=clf.datahandler.num_classes,\n","#     kl_divergence_fn=kl_divergence_fn,\n","#     task=clf.task)\n","\n","# =============================================================================\n","\n","# Alternative: Build model from available models in models.py\n","# =============================================================================\n","# clf.model = models.ProbabilisticClassificationCNN2D(\n","#     clf.datahandler.input_shape,\n","#     clf.datahandler.num_classes,\n","#     kl_divergence_fn,\n","#     bias_divergence_fn)\n","# ============================================================================="]},{"cell_type":"markdown","metadata":{"id":"KE4tBNhL7-Bu"},"source":["### Test models and fit calls (delete later)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FwbLHIpGLW35"},"outputs":[],"source":["def normal_sp(params):\n","  no_of_params = int(params.shape[-1]/2)\n","  loc = params[:,:no_of_params]\n","  scale = 1e-3 + tf.math.softplus(0.05 * params[:,no_of_params:])\n","  return tfd.Normal(loc=loc, scale=scale)\n","\n","class BayesianVICNN(models.EmptyModel):\n","    \"\"\"\n","    A CNN with three convolutional layers of different kernel size at\n","    the beginning. Works well for learning across scales.\n","\n","    \"\"\"\n","    def __init__(\n","        self,\n","        inputshape,\n","        num_classes,\n","        kl_divergence_fn,\n","        task,\n","        ):\n","        if len(inputshape) == 2:\n","            conv_layer = tfp.layers.Convolution1DFlipout\n","            strides = 1\n","            average_pool_layer = layers.AveragePooling1D\n","        elif len(inputshape) == 3:\n","            conv_layer = tfp.layers.Convolution2DFlipout\n","            strides = (1,1)\n","            average_pool_layer =  layers.AveragePooling2D\n","\n","        if (task == \"regression\" or task == \"multi_class_detection\"):\n","            if num_classes == 1:\n","                output_act = None\n","            else:\n","                output_act = \"sigmoid\"\n","\n","        elif task == \"classification\":\n","            output_act = \"softmax\"\n","\n","        ## Change activation in Bayesian layers?\n","        prob_act = \"relu\"\n","\n","        self.input_1 = layers.Input(\n","            shape = inputshape,\n","            name=\"input_1\")\n","        self.conv_1_short = conv_layer(\n","            filters=12,\n","            kernel_size=5,\n","            strides=strides,\n","            padding=\"same\",\n","            kernel_divergence_fn=kl_divergence_fn,\n","            bias_divergence_fn=kl_divergence_fn,\n","            activation=prob_act,\n","            name=\"conv_1_short\")(self.input_1)\n","        self.conv_1_medium = conv_layer(\n","            filters=12,\n","            kernel_size=10,\n","            strides=strides,\n","            padding=\"same\",\n","            kernel_divergence_fn=kl_divergence_fn,\n","            bias_divergence_fn=kl_divergence_fn,\n","            activation=prob_act,\n","            name=\"conv_1_medium\")(self.input_1)\n","        self.conv_1_long = conv_layer(\n","            filters=12,\n","            kernel_size=15,\n","            strides=strides,\n","            padding=\"same\",\n","            kernel_divergence_fn=kl_divergence_fn,\n","            bias_divergence_fn=kl_divergence_fn,\n","            activation=prob_act,\n","            name=\"conv_1_long\")(self.input_1)\n","\n","        sublayers = [self.conv_1_short, self.conv_1_medium, self.conv_1_long]\n","        merged_sublayers = layers.concatenate(sublayers)\n","\n","        self.conv_2 = conv_layer(\n","            filters=10,\n","            kernel_size=5,\n","            strides=strides,\n","            padding=\"valid\",\n","            kernel_divergence_fn=kl_divergence_fn,\n","            bias_divergence_fn=kl_divergence_fn,\n","            activation=prob_act,\n","            name=\"conv_2\")(merged_sublayers)\n","        self.conv_3 = conv_layer(\n","            filters=10,\n","            kernel_size=5,\n","            strides=strides,\n","            padding=\"valid\",\n","            kernel_divergence_fn=kl_divergence_fn,\n","            bias_divergence_fn=kl_divergence_fn,\n","            activation=prob_act,\n","            name=\"conv_3\")(self.conv_2)\n","        self.average_pool_1 = average_pool_layer(\n","            name=\"average_pool_1\")(self.conv_3)\n","\n","        self.flatten_1 = layers.Flatten(name=\"flatten1\")(self.average_pool_1)\n","        self.drop_1 = layers.Dropout(\n","            rate=0.2,\n","            name=\"drop_1\")(self.flatten_1)\n","        self.dense_1 = tfp.layers.DenseFlipout(\n","            units=4000,\n","            kernel_divergence_fn=kl_divergence_fn,\n","            bias_divergence_fn=kl_divergence_fn,\n","            activation=prob_act,\n","            name=\"dense_1\")(self.flatten_1)\n","\n","        self.dense_2 = tfp.layers.DenseFlipout(\n","            units=num_classes+num_classes,\n","            activation=output_act,\n","            kernel_divergence_fn=kl_divergence_fn,\n","            bias_divergence_fn=kl_divergence_fn)(self.dense_1)\n","        self.outputs = tfp.layers.DistributionLambda(normal_sp)(self.dense_2)\n","\n","        no_of_inputs = len(sublayers)\n","\n","        super(BayesianVICNN, self).__init__(\n","            inputs=self.input_1,\n","            outputs=self.outputs,\n","            inputshape=inputshape,\n","            num_classes=num_classes,\n","            no_of_inputs=no_of_inputs,\n","            name=\"BayesianVICNN\")\n","\n","model_vi = BayesianVICNN(\n","    inputshape=clf.datahandler.input_shape,\n","    num_classes=clf.datahandler.num_classes,\n","    kl_divergence_fn=kl_divergence_fn,\n","    task=clf.task)\n","model_vi.compile(Adam(learning_rate=0.01), loss=NLL, metrics=[\"accuracy\"])\n","\n","features_train = clf.datahandler.X_train[:20]\n","labels_train = clf.datahandler.y_train[:20]\n","\n","features_val = clf.datahandler.X_val[:5]\n","labels_val = clf.datahandler.y_val[:5]\n","\n","hist =  model_vi.fit(features_train,\n","                     labels_train,\n","                     epochs=5000,\n","                     verbose=1,\n","                     validation_data=(features_val, labels_val),\n","                     batch_size=512)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"chkF4_8Ti8KQ"},"outputs":[],"source":["features_test = clf.datahandler.X_test[:10]\n","labels_test = clf.datahandler.y_test[:10]\n","\n","predictions = np.array([model_vi.predict(features_test) for i in range(50)])\n","predictions.mean(axis=0)\n","predictions.std(axis=0)#.shape\n","#labels_test"]},{"cell_type":"markdown","metadata":{"id":"kZ5JOdivbbJc"},"source":["#### Design loss "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tzgwBq24baBe"},"outputs":[],"source":["# =============================================================================\n","# def _neg_log_likelihood_bayesian(y_true, y_pred):\n","#     labels_distribution = tfp.distributions.OneHotCategorical(logits=y_pred)\n","#     return -tf.reduce_mean(labels_distribution.log_prob(y_true))\n","# =============================================================================\n","\n","# =============================================================================\n","# def loss_fn(y_pred, y_true):\n","#     return tf.nn.sigmoid_cross_entropy_with_logits(labels=y_true,logits=y_pred)\n","# =============================================================================\n","\n","# =============================================================================\n","# def NLL(y_true, y_pred):\n","#     # Since y_pred is distribution object, we can call log_prob for sample data\n","#     return -y_pred.log_prob(y_true)\n","# =============================================================================\n","\n","# =============================================================================\n","# def neg_log_likelihood(y_obs, y_pred, sigma=noise):\n","#     dist = tfp.distributions.Normal(loc=y_pred, scale=sigma)\n","#     return K.sum(-dist.log_prob(y_obs))\n","# =============================================================================\n","## How to model sigma?"]},{"cell_type":"markdown","metadata":{"id":"I2SpBC-R9XsA"},"source":["### Compile and summarize the model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vKQZDayn9XsB"},"outputs":[],"source":["from tensorflow.keras.optimizers import Adam\n","import tensorflow.keras.losses as tf_losses\n","import tensorflow.keras.metrics as tf_metrics\n","\n","learning_rate = 1e-05\n","optimizer = Adam(learning_rate = learning_rate)\n","\n","if clf.task == \"regression\":\n","    #loss = tf_losses.MeanAbsoluteError()\n","    #metrics = [tf_metrics.MeanSquaredError(name=\"mse\")]\n","    loss = tf_losses.MeanSquaredError()\n","    metrics = [tf_metrics.MeanAbsoluteError(name=\"mae\")]\n","\n","elif clf.task == \"classification\":\n","    loss = tf_losses.CategoricalCrossentropy()\n","    metrics = [tf_metrics.CategoricalCrossentropy(name=\"accuracy\")]\n","\n","elif clf.task == \"multi_class_detection\":\n","    loss =  tf_losses.BinaryCrossentropy()\n","    metrics = [tf_metrics.BinaryAccuracy(name=\"accuracy\", threshold=0.7)]\n","\n","clf.model.compile(loss=loss,\n","                  optimizer=optimizer,\n","                  metrics=metrics)\n","\n","# Plot summary and save model plot.\n","clf.summary()\n","clf.save_and_print_model_image()"]},{"cell_type":"markdown","metadata":{"id":"ukB2YPfgRjyS"},"source":["### Show initial weight distributions"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yzktfypsN3ud"},"outputs":[],"source":["clf.plot_weight_distribution(kind=\"prior\", to_file=True)\n","clf.plot_weight_distribution(kind=\"posterior\", to_file=True)"]},{"cell_type":"markdown","metadata":{"id":"oeboNYkNsf7Z"},"source":["### Show initial predictions"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RO4qrjPX0pFU"},"outputs":[],"source":["no_of_predictions = 10\n","\n","print(\"Train:\")\n","prob_pred_train_initial = clf.predict_probabilistic(\n","    dataset=\"train\",\n","    no_of_predictions=no_of_predictions\n",")\n","\n","for i, pred in enumerate(prob_pred_train_initial[:10]):\n","   if i < clf.datahandler.y_train.shape[0]:\n","       print(f\"Ground truth: {np.round(clf.datahandler.y_train[i],3)},\",\n","             f\"Mean prediction: {np.mean(pred, axis = 0)} +/- {np.std(pred, axis = 0)}\")\n","\n","print(\"Test:\")\n","prob_pred_test_initial = clf.predict_probabilistic(\n","    dataset=\"test\",\n","    no_of_predictions=no_of_predictions\n",")\n","\n","for i, pred in enumerate(prob_pred_test_initial[:10]):\n","   if i < clf.datahandler.y_test.shape[0]:\n","       print(f\"Ground truth: {np.round(clf.datahandler.y_test[i],3)},\",\n","             f\"Mean prediction: {np.mean(pred, axis = 0)} +/- {np.std(pred, axis = 0)}\")"]},{"cell_type":"markdown","metadata":{"id":"GDOgVS_bLwGC"},"source":["### Train"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"abV_fFCb5fiZ"},"outputs":[],"source":["epochs = 250\n","batch_size = 32\n","validation_freq = 1\n","\n","hist = clf.train(checkpoint=True,\n","                 early_stopping=False,\n","                 tb_log=True,\n","                 csv_log=True,\n","                 hyperparam_log=True,\n","                 #cb_parameters={\"es_patience\":15,},\n","                 epochs=epochs,\n","                 batch_size=batch_size,\n","                 validation_freq=validation_freq,\n","                 verbose=2)\"\""]},{"cell_type":"markdown","metadata":{"id":"J9uYqsMlLwGJ"},"source":["### Plot loss"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ffr2nfZy5nYi"},"outputs":[],"source":["graph = clfutils.TrainingGraphs(clf.logging.history, clf.logging.fig_dir)\n","graph.plot_loss(to_file = True)\n","if clf.task != \"regression\":\n","    graph.plot_accuracy(to_file = False)"]},{"cell_type":"markdown","metadata":{"id":"jnAnvSXLLwGQ"},"source":["### Evaluate on test data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4g_QmUYG5fuT","scrolled":true},"outputs":[],"source":["if clf.task == \"regression\":\n","    test_loss = clf.evaluate()\n","    print(\"Test loss: \" + str(np.round(test_loss, decimals=8)))\n","\n","else:\n","    score = clf.evaluate()\n","    test_loss, test_accuracy = score[0], score[1]\n","    print(\"Test loss: \" + str(np.round(test_loss, decimals=8)))\n","    print(\"Test accuracy: \" + str(np.round(test_accuracy, decimals=3)))"]},{"cell_type":"markdown","metadata":{"id":"2uCoVCI-LwGY"},"source":["###  Predict on train and test data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AYyRZWgcumRJ"},"outputs":[],"source":["no_of_predictions = 10\n","\n","print(\"Train:\")\n","prob_pred_train = clf.predict_probabilistic(\n","    dataset=\"train\",\n","    no_of_predictions=no_of_predictions\n",")\n","\n","for i, pred in enumerate(prob_pred_train[:10]):\n","   print(f\"Ground truth: {np.round(clf.datahandler.y_train[i],3)},\",\n","         f\"Mean prediction: {np.mean(pred, axis = 0)} +/- {np.std(pred, axis = 0)}\")\n","\n","print(\"Test:\")\n","prob_pred_test = clf.predict_probabilistic(\n","    dataset=\"test\",\n","    no_of_predictions=no_of_predictions\n",")\n","\n","for i, pred in enumerate(prob_pred_test[:10]):\n","   print(f\"Ground truth: {np.round(clf.datahandler.y_test[i],3)},\",\n","         f\"Mean prediction: {np.mean(pred, axis = 0)} +/- {np.std(pred, axis = 0)}\")"]},{"cell_type":"markdown","metadata":{"id":"nWbogghiLwGl"},"source":["### Show some predictions"]},{"cell_type":"markdown","metadata":{"id":"T5DGB_OGLwGm"},"source":["#### 10 random training samples"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"keDKIJriLwGn"},"outputs":[],"source":["clf.plot_random(no_of_spectra=10, dataset=\"train\", with_prediction=True)"]},{"cell_type":"markdown","metadata":{"id":"-PHAS9XILwGr"},"source":["#### 10 random test samples"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HM3ZZf-qLwGs"},"outputs":[],"source":["clf.plot_random(no_of_spectra=10, dataset=\"test\", with_prediction=True)"]},{"cell_type":"markdown","metadata":{"id":"ROK1zo8xzBZF"},"source":["### Show wrong/worst predictions"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"both","id":"uHFC5PWQzE19"},"outputs":[],"source":["if clf.task == \"classification\":\n","    clf.show_wrong_classification()\n","else:\n","    clf.show_worst_predictions(no_of_spectra = 20)"]},{"cell_type":"markdown","metadata":{"id":"ii2umXAZuPZI"},"source":["### Show posterior weight distribution after training update"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wcvroHH-Tb8x"},"outputs":[],"source":["clf.plot_weight_distribution(kind=\"prior\", to_file=True)\n","clf.plot_weight_distribution(kind=\"posterior\", to_file=True)"]},{"cell_type":"markdown","metadata":{"id":"wp6sYutg942J"},"source":["### Show distribution of probabilistic predictions"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ez4DC9SUUszH"},"outputs":[],"source":["clf.plot_prob_predictions(dataset=\"test\",\n","                          no_of_spectra=10,\n","                          to_file=True)"]},{"cell_type":"markdown","metadata":{"id":"KwrgvaLpLwG3"},"source":["### Save model and results"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2HBdnZSq5f2D"},"outputs":[],"source":["#clf.save_model()\n","clf.pickle_results()"]},{"cell_type":"markdown","metadata":{"id":"c6B2wITlLwG_"},"source":["### Generate report"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-rPXD0ki5pOp"},"outputs":[],"source":["dir_name = clf.time + \"_\" + clf.exp_name\n","rep = clfutils.Report(dir_name)\n","rep.write()"]},{"cell_type":"markdown","metadata":{"id":"K4mJKWJxWllD"},"source":["## Continue training"]},{"cell_type":"markdown","metadata":{"id":"VH95P7yXcCTq"},"source":["### Load custom modules"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ILECvnh6cCTr"},"outputs":[],"source":["try:\n","    import importlib\n","    importlib.reload(classifier)\n","    importlib.reload(clfutils)\n","    print(\"\\n Modules were reloaded.\")\n","except:\n","    import xpsdeeplearning.network.classifier as classifier\n","    import xpsdeeplearning.network.utils as clfutils\n","    print(\"Modules were loaded.\")"]},{"cell_type":"markdown","metadata":{"id":"qu-oYRDYv93B"},"source":["### Reload classifier from previous run"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fffC8Ch1stjm"},"outputs":[],"source":["runpath = r\"/content/drive/My Drive/deepxps/runs/20220118_14h46m_Ni_2_classes_long_linear_comb_small_gas_phase_multi_class_detection_CNN_bayesian_relu\"\n","clf = classifier.restore_clf_from_logs(runpath)"]},{"cell_type":"markdown","metadata":{"id":"hO_5SSXX0O7I"},"source":["### Load and inspect the data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bXXFcFUCWxFI"},"outputs":[],"source":["X_train, X_val, X_test, y_train, y_val, y_test,\\\n","    sim_values_train, sim_values_val, sim_values_test =\\\n","        clf.load_data_preprocess(input_filepath=clf.logging.hyperparams[\"input_filepath\"],\n","                                 no_of_examples=clf.logging.hyperparams[\"no_of_examples\"],\n","                                 train_test_split=clf.logging.hyperparams[\"train_test_split\"],\n","                                 train_val_split=clf.logging.hyperparams[\"train_val_split\"])\n","\n","# Check how the examples are distributed across the classes.\n","class_distribution = clf.datahandler.check_class_distribution(clf.task)\n","clf.plot_class_distribution()\n","clf.plot_random(no_of_spectra = 10, dataset = \"train\")"]},{"cell_type":"markdown","metadata":{"id":"j1lFa5GDz9dz"},"source":["#### Other data"]},{"cell_type":"markdown","metadata":{"id":"LACmhmalz9dz"},"source":["##### Only use classification data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HKoJeH7Yz9dz"},"outputs":[],"source":["### Only use classification data\n","if clf.task == \"classification\":\n","    X_train, X_val, X_test, y_train, y_val, y_test, \\\n","        sim_values_train, sim_values_val, sim_values_test =\\\n","            clf.datahandler._only_keep_classification_data()\n","\n","    clf.plot_random(no_of_spectra = 10, dataset = \"train\")\n","\n","elif clf.task == \"regression\":\n","    print(\"Dataset was not changed.\")"]},{"cell_type":"markdown","metadata":{"id":"ZNfu-1Fjz9dz"},"source":["##### MNIST"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-SFM_bKhz9d0"},"outputs":[],"source":["### Loads MNIST dataset.###\n","import matplotlib.pyplot as plt\n","clf.datahandler.train_test_split = 0.1\n","clf.datahandler.train_val_split = 0.1\n","clf.datahandler.no_of_examples = 4000\n","\n","print(\"Loading MNIST dataset\")\n","(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()\n","\n","X = np.expand_dims(\n","    np.concatenate((X_train, X_test), axis=0),\n","    -1)[:no_of_examples]\n","X = X.astype(\"float32\") / 255\n","y = np.expand_dims(\n","    np.concatenate((y_train, y_test), axis=0),\n","    -1)[:no_of_examples]\n","clf.datahandler.X, clf.datahandler.y = X, y\n","\n","(\n","    X_train,\n","    X_val,\n","    X_test,\n","    y_train,\n","    y_val,\n","    y_test,\n"," ) = clf.datahandler._split_test_val_train(X, y)\n","\n","clf.datahandler.X_train, clf.datahandler.y_train = X_train, y_train\n","clf.datahandler.X_val, clf.datahandler.y_val = X_val, y_val\n","clf.datahandler.X_test, clf.datahandler.y_test = X_test, y_test\n","clf.datahandler.input_shape = (clf.datahandler.X_train.shape[1:])\n","clf.datahandler.num_classes = 10\n","clf.datahandler.labels = list(range(clf.datahandler.num_classes))\n","\n","print(\"X_train.shape =\", X_train.shape)\n","print(\"y_train.shape =\", y_train.shape)\n","print(\"X_val.shape =\", X_val.shape)\n","print(\"y_val.shape =\", y_val.shape)\n","print(\"X_test.shape =\", X_test.shape)\n","print(\"y_test.shape =\", y_test.shape)\n","\n","plt.imshow(X_train[0, :, :, 0], cmap=\"gist_gray\")\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"dCpcVGI0z9d0"},"source":["##### Shrunken babys"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"u4k6wbYYz9d0"},"outputs":[],"source":["# Loads shrunken baby dataset\n","from PIL import Image\n","import glob\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","\n","clf.datahandler.train_test_split = 0.1\n","clf.datahandler.train_val_split = 0.1\n","clf.datahandler.no_of_examples = 40\n","num_classes = 10\n","\n","image_paths = glob.glob(r\"/content/drive/My Drive/deepxps/datasets/shrunken_baby_ds/*.png\")\n","\n","X = [np.array(Image.open(im)) for im in image_paths[:clf.datahandler.no_of_examples]]\n","X = [np.expand_dims(image, -1) for image in X] # add extra dimension to each image (126,126) --> (126,126,1)\n","X = np.array(X) # convert list of images to single array [(126,126,1)] --> (836, 126, 126, 1)\n","\n","from skimage.measure import block_reduce\n","#X = X[:,5:-21,13:-13,:]\n","X_new = []\n","for image in X:\n","    reduced_image = block_reduce(image,\n","                                 block_size=(2, 2, 1),\n","                                 func=np.mean)\n","    X_new.append(reduced_image)\n","X_new = np.array(X_new)\n","\n","# Normalize data\n","X = X.astype(\"float32\") / 255\n","X_new = X_new.astype(\"float32\") / 255\n","\n","\n","#y_old = pd.read_csv(r\"/content/drive/My Drive/deepxps/datasets/shrunken_baby_ds/shrunken_baby_labels.csv\").to_numpy()[:clf.datahandler.no_of_examples]\n","\n","#y = pd.read_csv(r\"/content/drive/My Drive/deepxps/datasets/shrunken_baby_ds/shrunken_baby_labels_new.csv\").to_numpy()[:clf.datahandler.no_of_examples]\n","child_positions = [np.where(X[i] > 0.2) for i in range(clf.datahandler.no_of_examples)]\n","y = np.array([np.max(child_positions[i][0]) - np.min(child_positions[i][0]) for i in range(clf.datahandler.no_of_examples)])\n","y = np.expand_dims(y, -1)\n","\n","clf.datahandler.X, clf.datahandler.y = X_new, y\n","\n","(\n","    X_train,\n","    X_val,\n","    X_test,\n","    y_train,\n","    y_val,\n","    y_test,\n"," ) = clf.datahandler._split_test_val_train(X_new, y)\n","\n","# Train-test split\n","#num_train_val = int((1-test_split)*X.shape[0])\n","#(X_train_val, X_test) = X[:num_train_val], X[num_train_val:]\n","#(y_train_val, y_test) = y[:num_train_val], y[num_train_val:]\n","\n","# Train-val split\\n\",\"num_train = int((1-val_split)*X_train_val.shape[0])\n","#(X_train, X_val) = X_train_val[:num_train], X_train_val[num_train:]\n","#(y_train, y_val) = y_train_val[:num_train], y_train_val[num_train:]\n","\n","clf.datahandler.X_train, clf.datahandler.y_train = X_train, y_train\n","clf.datahandler.X_val, clf.datahandler.y_val = X_val, y_val\n","clf.datahandler.X_test, clf.datahandler.y_test = X_test, y_test\n","clf.datahandler.input_shape = (clf.datahandler.X_train.shape[1:])\n","clf.datahandler.num_classes = 1\n","clf.datahandler.labels = [\"sizes\"]\n","\n","print(\"X.shape =\", clf.datahandler.X.shape)\n","print(\"y.shape =\", clf.datahandler.y.shape)\n","print(\"X_train.shape =\", clf.datahandler.X_train.shape)\n","print(\"y_train.shape =\", clf.datahandler.y_train.shape)\n","print(\"X_val.shape =\", clf.datahandler.X_val.shape)\n","print(\"y_val.shape =\", clf.datahandler.y_val.shape)\n","print(\"X_test.shape =\", clf.datahandler.X_test.shape)\n","print(\"y_test.shape =\", clf.datahandler.y_test.shape)\n","print(\"\\n\")\n","\n","# shrunk = []\n","# for i in range(clf.datahandler.no_of_examples):\n","#     child_position = np.where(X_new[i] > 0.2)\n","#     min_height, max_height = np.min(child_position[0]), np.max(child_position[0])\n","#     shrunk.append(max_height - min_height)\n","# shrunk = np.expand_dims(np.array(shrunk),-1)\n","\n","for i in range(5):\n","    r = i#np.random.randint(0,X.shape[0])\n","    fig, ax = plt.subplots(nrows=1, ncols=2)\n","\n","    child_position = np.where(X[r] > 0.2)\n","    min_height, max_height = np.min(child_position[0]), np.max(child_position[0])\n","    min_height_hor = child_position[1][np.argmin(child_position[0])]\n","    max_height_hor = child_position[1][np.argmax(child_position[0])]\n","    child_position_new = np.where(X_new[r] > 50/255.0)\n","    min_height_new, max_height_new = np.min(child_position_new[0]), np.max(child_position_new[0])\n","    min_height_hor_new = child_position_new[1][np.argmin(child_position_new[0])]\n","    max_height_hor_new = child_position_new[1][np.argmax(child_position_new[0])]\n","    real_size = int(y[r])\n","    shrunk_size = max_height_new - min_height_new\n","    print(f\"Child no. {r}, real size: {real_size}, new size: {shrunk_size}, factor: {np.round(real_size/shrunk_size,2)}\")\n","\n","    _ = ax[0].imshow(np.squeeze(X[r]))\n","    _ = ax[0].scatter(min_height_hor, min_height,  s=50, c=\"red\", marker=\".\")\n","    _ = ax[0].scatter(max_height_hor, max_height,  s=50, c=\"blue\", marker=\".\")\n","    _ = ax[1].imshow(np.squeeze(X_new[r]))\n","    _ = ax[1].scatter(min_height_hor_new, min_height_new,  s=50, c=\"red\", marker=\".\")\n","    _ = ax[1].scatter(max_height_hor_new, max_height_new,  s=50, c=\"blue\", marker=\".\")\n","    plt.show()"]},{"cell_type":"markdown","metadata":{"id":"Lqkf3HvZz9d1"},"source":["##### Housing prices"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"o9y3A2LFz9d1"},"outputs":[],"source":["clf.datahandler.train_test_split = 0.1\n","clf.datahandler.train_val_split = 0.1\n","clf.datahandler.no_of_examples = 100\n","\n","from keras.datasets import boston_housing\n","(X_train, y_train), (X_test, y_test) = boston_housing.load_data()\n","\n","X = np.expand_dims(\n","    np.concatenate((X_train, X_test), axis=0),\n","    -1)[:no_of_examples]\n","y = np.expand_dims(\n","    np.concatenate((y_train, y_test), axis=0),\n","    -1)[:no_of_examples]\n","clf.datahandler.X, clf.datahandler.y = X, y\n","\n","(\n","    X_train,\n","    X_val,\n","    X_test,\n","    y_train,\n","    y_val,\n","    y_test,\n"," ) = clf.datahandler._split_test_val_train(X, y)\n","\n","\n","clf.datahandler.X_train, clf.datahandler.y_train = X_train, y_train\n","clf.datahandler.X_val, clf.datahandler.y_val = X_val, y_val\n","clf.datahandler.X_test, clf.datahandler.y_test = X_test, y_test\n","clf.datahandler.input_shape = (clf.datahandler.X_train.shape[1:])\n","clf.datahandler.num_classes = 1\n","clf.datahandler.labels = [\"prizes\"]\n","\n","print(\"X_train.shape =\", X_train.shape)\n","print(\"y_train.shape =\", y_train.shape)\n","print(\"X_val.shape =\", X_val.shape)\n","print(\"y_val.shape =\", y_val.shape)\n","print(\"X_test.shape =\", X_test.shape)\n","print(\"y_test.shape =\", y_test.shape)"]},{"cell_type":"markdown","metadata":{"id":"BqJ1CeG1WxFM"},"source":["### Load the model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JvzitnVNWxFR"},"outputs":[],"source":["### Currently not working, does not load prior/posterior distributions ####\n","from tensorflow.python.keras import backend as K\n","clf.load_model(compile_model = True)\n","### Come back later to check on this ###"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TuJn1U4oKM1z"},"outputs":[],"source":["### Current alternative ###\n","# Use the same model defined above and load the weights independently\n","# Need to run the cell with the definition of the model class above once\n","model_class = BayesianCNN # CHANGE HERE\n","\n","kl_divergence_fn = (\n","    lambda q, p, _: (tfp.distributions.kl_divergence(q, p)) /\n","    tf.cast(clf.datahandler.X_train.shape[0], dtype=tf.float32)\n","    )\n","\n","clf.model = model_class(inputshape=clf.datahandler.input_shape,\n","                        num_classes=clf.datahandler.num_classes,\n","                        kl_divergence_fn=kl_divergence_fn,\n","                        task=clf.task)\n","\n","# LOAD WEIGHTS\n","weights_file = os.path.join(clf.logging.model_dir,\n","                            \"weights.h5\")\n","clf.model.load_weights(weights_file)\n","\n","# Compile and summarize the model\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.losses import MeanAbsoluteError, MeanSquaredError, CategoricalCrossentropy, BinaryCrossentropy\n","from tensorflow.keras.metrics import MeanSquaredError, CategoricalCrossentropy, BinaryAccuracy\n","\n","learning_rate = clf.logging.hyperparams[\"learning_rate\"]\n","optimizer = Adam(learning_rate = learning_rate)\n","\n","if clf.task == \"regression\":\n","    loss = MeanAbsoluteError()\n","    #loss = MeanSquaredError()\n","    metrics=[MeanSquaredError(name=\"mse\")]\n","\n","elif clf.task == \"classification\":\n","    loss = CategoricalCrossentropy()\n","    metrics = [CategoricalCrossentropy(name=\"accuracy\")]\n","\n","elif clf.task == \"multi_class_detection\":\n","    loss = BinaryCrossentropy()\n","    metrics = metrics = [BinaryAccuracy(name=\"accuracy\", threshold=0.7)]\n","\n","clf.model.compile(loss=loss,\n","                  optimizer=optimizer,\n","                  metrics=metrics)"]},{"cell_type":"markdown","metadata":{"id":"51UVlZILWxFW"},"source":["### Summarize the model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KN24emZEWxFW"},"outputs":[],"source":["# Plot summary and save model plot.\n","clf.summary()\n","clf.save_and_print_model_image()"]},{"cell_type":"markdown","metadata":{"id":"lFt_ILZ9EnDn"},"source":["### Show current weight distributions"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"N-OgsDz2EnDn"},"outputs":[],"source":["clf.plot_weight_distribution(kind=\"prior\", to_file=True)\n","clf.plot_weight_distribution(kind=\"posterior\", to_file=True)"]},{"cell_type":"markdown","metadata":{"id":"hr2aMkdaqg2K"},"source":["### Show predictions with current model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Z1pcEkr7EnDo"},"outputs":[],"source":["no_of_predictions = 10\n","\n","print(\"Train:\")\n","prob_pred_train_intermediate = clf.predict_probabilistic(\n","    dataset=\"train\",\n","    no_of_predictions=no_of_predictions\n",")\n","\n","for i, pred in enumerate(prob_pred_train_intermediate[:10]):\n","   print(f\"Ground truth: {np.round(clf.datahandler.y_train[i],3)},\",\n","         f\"Mean prediction: {np.mean(pred, axis = 0)} +/- {np.std(pred, axis = 0)}\")\n","\n","print(\"Test:\")\n","prob_pred_test_intermediate = clf.predict_probabilistic(\n","    dataset=\"test\",\n","    no_of_predictions=no_of_predictions\n",")\n","\n","for i, pred in enumerate(prob_pred_test_intermediate[:10]):\n","   print(f\"Ground truth: {np.round(clf.datahandler.y_test[i],3)},\",\n","         f\"Mean prediction: {np.mean(pred, axis = 0)} +/- {np.std(pred, axis = 0)}\")"]},{"cell_type":"markdown","metadata":{"id":"TcYkliU1WxFa"},"source":["### Train"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XVQWFw1yWxFa"},"outputs":[],"source":["epochs = 1000\n","\n","#new_learning_rate = 1e-05\n","\n","hist = clf.train(checkpoint=True,\n","                 early_stopping=False,\n","                 tb_log=True,\n","                 csv_log=True,\n","                 hyperparam_log=True,\n","                 epochs=epochs,\n","                 batch_size=clf.logging.hyperparams[\"batch_size\"],\n","                 verbose=2,)\n","                 #new_learning_rate=new_learning_rate)"]},{"cell_type":"markdown","metadata":{"id":"VlkNFbYNWxFe"},"source":["### Plot loss"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9txM4QFyWxFe"},"outputs":[],"source":["graph = clfutils.TrainingGraphs(clf.logging.history, clf.logging.fig_dir)\n","graph.plot_loss(to_file = True)\n","if clf.task == \"regression\":\n","    graph.plot_mse(to_file = True)\n","else:\n","    graph.plot_accuracy(to_file = False)"]},{"cell_type":"markdown","metadata":{"id":"OY7AxjWsWxFg"},"source":["### Evaluate on test data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6deyORg8stjo","scrolled":true},"outputs":[],"source":["if clf.task == \"regression\":\n","    test_loss = clf.evaluate()\n","    print(\"Test loss: \" + str(np.round(test_loss, decimals=8)))\n","\n","else:\n","    score = clf.evaluate()\n","    test_loss, test_accuracy = score[0], score[1]\n","    print(\"Test loss: \" + str(np.round(test_loss, decimals=8)))\n","    print(\"Test accuracy: \" + str(np.round(test_accuracy, decimals=3)))"]},{"cell_type":"markdown","metadata":{"id":"sAlwSDZNstjo"},"source":["###  Predict on train and test data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UOsQWaBeFBNE"},"outputs":[],"source":["no_of_predictions = 100\n","\n","print(\"Train:\")\n","prob_pred_train_intermediate = clf.predict_probabilistic(\n","    dataset=\"train\",\n","    no_of_predictions=no_of_predictions\n",")\n","\n","for i, pred in enumerate(prob_pred_train_intermediate[:10]):\n","   print(f\"Ground truth: {np.round(clf.datahandler.y_train[i],3)},\",\n","         f\"Mean prediction: {np.mean(pred, axis = 0)} +/- {np.std(pred, axis = 0)}\")\n","\n","print(\"Test:\")\n","prob_pred_test_intermediate = clf.predict_probabilistic(\n","    dataset=\"test\",\n","    no_of_predictions=no_of_predictions\n",")\n","\n","for i, pred in enumerate(prob_pred_test_intermediate[:10]):\n","   print(f\"Ground truth: {np.round(clf.datahandler.y_test[i],3)},\",\n","         f\"Mean prediction: {np.mean(pred, axis = 0)} +/- {np.std(pred, axis = 0)}\")"]},{"cell_type":"markdown","metadata":{"id":"_oeZgBAkFBNE"},"source":["### Show some predictions"]},{"cell_type":"markdown","metadata":{"id":"bWWf9ORFFBNE"},"source":["#### 10 random training samples"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LCJXx9AvFBNE"},"outputs":[],"source":["clf.plot_random(no_of_spectra=10, dataset=\"train\", with_prediction=True)"]},{"cell_type":"markdown","metadata":{"id":"SmI9ySIqFBNE"},"source":["#### 10 random test samples"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YIlKNEcrFBNF"},"outputs":[],"source":["clf.plot_random(no_of_spectra=10, dataset=\"test\", with_prediction=True)"]},{"cell_type":"markdown","metadata":{"id":"vN1IvEy0FBNF"},"source":["### Show wrong/worst predictions"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"both","id":"pVwS0jiPFBNF"},"outputs":[],"source":["if clf.task == \"classification\":\n","    clf.show_wrong_classification()\n","elif clf.task == \"regression\":\n","    clf.show_worst_predictions(no_of_spectra=20)"]},{"cell_type":"markdown","metadata":{"id":"CCWyA2hSFBNF"},"source":["### Show posterior weight distribution after training update"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oQ0j20ezFBNF"},"outputs":[],"source":["clf.plot_weight_distribution(kind=\"posterior\", to_file=True)"]},{"cell_type":"markdown","metadata":{"id":"JGZhHmwVFBNF"},"source":["### Show distribution of probabilistic predictions"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xY7JJAQOFBNF"},"outputs":[],"source":["clf.plot_prob_predictions(dataset=\"test\",\n","                          no_of_spectra=20,\n","                          to_file=True)"]},{"cell_type":"markdown","metadata":{"id":"1laSfj9pfsGu"},"source":["### Plot updates to weight and bias uncertainties"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UMAVxT-GFe2X"},"outputs":[],"source":["init_model = BayesianCNN(\n","    inputshape=clf.datahandler.input_shape,\n","    num_classes=clf.datahandler.num_classes,\n","    kl_divergence_fn=kl_divergence_fn,\n","    task=clf.task)\n","\n","trained_model = CNN(\n","    inputshape=clf.datahandler.input_shape,\n","    num_classes=clf.datahandler.num_classes,\n","    task=clf.task)\n","\n","weights_file = os.path.join(os.getcwd(),\n","                            \"weights.h5\")\n","trained_model.load_weights(weights_file)\n","\n","init_bayesian_conv_layers = [layer for layer in init_model.layers if \"Conv1DFlipout\" in str(layer.__class__)]\n","trained_conv_layers = [layer for layer in trained_model.layers if \"Conv1D\" in str(layer.__class__)]\n","trained_bayesian_conv_layers = [layer for layer in clf.model.layers if \"Conv1DFlipout\" in str(layer.__class__)]\n","\n","init_bayesian_dense_layers = [layer for layer in init_model.layers if \"Dense\" in str(layer.__class__)]\n","trained_dense_layers = [layer for layer in trained_model.layers if \"Dense\" in str(layer.__class__)]\n","trained_bayesian_dense_layers = [layer for layer in clf.model.layers if \"Dense\" in str(layer.__class__)]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6nCm6TEJHqvk"},"outputs":[],"source":["## Plot weight uncertainties and updates\n","\n","import itertools\n","import seaborn as sns\n","import matplotlib.colors as mcolors\n","import matplotlib.pyplot as plt\n","\n","from matplotlib.cm import get_cmap\n","names = \"Set1\", \"Set2\", \"Accent\", \"Dark2\"\n","colors = []\n","for name in names:\n","    cmap = get_cmap(name)  # type: matplotlib.colors.ListedColormap\n","    colors.extend(cmap.colors)  # type: list\n","\n","nrows = len(init_bayesian_conv_layers)\n","ncols=12\n","\n","fig1, axs1 = plt.subplots(nrows, ncols, figsize=(40,20))\n","fig2, axs2 = plt.subplots(nrows, ncols, figsize=(40,10))\n","\n","for l, (layer_init, layer_trained, bayesian_layer_trained) in enumerate(zip(init_bayesian_conv_layers, trained_conv_layers, trained_bayesian_conv_layers)):\n","    print(f\"\\n {layer_init.name}\")\n","    posterior_mean_init = layer_init.kernel_posterior.mean().numpy().transpose(2,1,0)\n","    posterior_stddev_init = layer_init.kernel_posterior.stddev().numpy().transpose(2,1,0)\n","\n","    posterior_mean_trained = bayesian_layer_trained.kernel_posterior.mean().numpy().transpose(2,1,0)\n","    posterior_stddev_trained = bayesian_layer_trained.kernel_posterior.stddev().numpy().transpose(2,1,0)\n","\n","    kernel_trained = layer_trained.get_weights()[0].transpose(2,1,0)\n","\n","    for f, (posterior_filter_mean_init,\n","            posterior_filter_stddev_init,\n","            posterior_filter_mean_trained,\n","            posterior_filter_stddev_trained,\n","            ) in enumerate(zip(posterior_mean_init, posterior_stddev_init, posterior_mean_trained, posterior_stddev_trained)):\n","        print(f\"Filter {f}\")\n","        mean_posterior_kernel_mean_init = float(np.mean(posterior_filter_mean_init))\n","        mean_posterior_kernel_stddev_init = float(np.mean(posterior_filter_stddev_init))\n","\n","        mean_posterior_kernel_mean_trained = float(np.mean(posterior_filter_mean_trained))\n","        mean_posterior_kernel_stddev_trained = float(np.mean(posterior_filter_stddev_trained))\n","\n","        weight_dist_init = tfp.distributions.Normal(loc=mean_posterior_kernel_mean_init, scale=mean_posterior_kernel_stddev_init)\n","        weight_dist_trained = tfp.distributions.Normal(loc=mean_posterior_kernel_mean_trained, scale=mean_posterior_kernel_stddev_trained)\n","\n","        color_iter = itertools.cycle(colors)\n","\n","        _ = sns.histplot(weight_dist_init.sample(10000), ax=axs1[l,f], bins=50, color=\"b\", kde=True)\n","        _ = sns.histplot(weight_dist_trained.sample(10000), ax=axs1[l,f], bins=50, color=\"r\", kde=True)\n","\n","        _ = axs1[l,f].set_xlim([-3,3])\n","        _ = axs1[l,f].set_title(f\"{layer_init.name}, f{f}\", fontdict={\"fontsize\": 10})\n","        _ = axs1[l,f].legend([\"init\", \"trained\"])\n","\n","        for k, (kernel_mean_init, kernel_stddev_init, kernel_mean_trained,  kernel_stddev_trained) in enumerate(zip(posterior_filter_mean_init, posterior_filter_stddev_init, posterior_filter_mean_trained, posterior_filter_stddev_trained)):\n","            c = next(color_iter)\n","            trained_weight = np.mean(kernel_trained[f], axis=1)[k]\n","            _ = axs2[l, f].plot(np.array([kernel_mean_init, kernel_mean_trained]), np.array([0,1]), color=c)\n","            _ = axs2[l, f].scatter(trained_weight, np.array(2), color=c)\n","            _ = axs2[l,f].set_title(f\"{layer_init.name}, f{f}\", fontdict={\"fontsize\": 10})\n","            _ = axs2[l,f].set_yticks([0, 1, 2])\n","            _ = axs2[l, f].set_yticklabels([\"init\", \"trained_bayesian\",\"trained\"])\n","            #print(f\"Kernel {k}: Init posterior:  {kernel_mean_init}  {kernel_stddev_init}, Trained posterior:  {kernel_mean_trained}  {kernel_stddev_trained}\")\n","\n","        #print(f\"Init posterior:  {posterior_filter_mean_init}  {posterior_filter_stddev_init}, Trained posterior:  {posterior_filter_mean_trained}  {posterior_filter_stddev_trained}\")\n","        print(f\"Init posterior:  {mean_posterior_kernel_mean_init}  {mean_posterior_kernel_stddev_init}, Trained posterior:  {mean_posterior_kernel_mean_trained}  {mean_posterior_kernel_stddev_trained}\")\n","\n","print(\"Fig. 1: Samples from posterior kernel distributions (averaged across filter)\")\n","print(\"Fig. 2: Shift of posterior kernel means after training.\")\n","\n","fig1.tight_layout()\n","fig2.tight_layout()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uEnPvgKNXxte"},"outputs":[],"source":["## Plot bias uncertainties and updates\n","\n","import matplotlib.colors as mcolors\n","import matplotlib.pyplot as plt\n","\n","from matplotlib.cm import get_cmap\n","names = \"Set1\", \"Set2\"\n","colors = []\n","for name in names:\n","    cmap = get_cmap(name)  # type: matplotlib.colors.ListedColormap\n","    colors.extend(cmap.colors)  # type: list\n","\n","no_of_layers = len(init_bayesian_conv_layers)\n","\n","fig3, axs3 = plt.subplots(nrows=no_of_layers, ncols=12, figsize=(20,10))\n","fig4, axs4 = plt.subplots(nrows=1, ncols=no_of_layers, figsize=(20,5))\n","\n","for l, (layer_init, layer_trained, bayesian_layer_trained) in enumerate(zip(init_bayesian_conv_layers, trained_conv_layers, trained_bayesian_conv_layers)):\n","    print(f\"\\n {layer_init.name}\")\n","\n","    biases_trained = layer_trained.get_weights()[1]\n","\n","    posterior_mean_init = layer_init.bias_posterior.mean().numpy()\n","    posterior_stddev_init = layer_init.bias_posterior.stddev().numpy()\n","\n","    posterior_mean_trained = bayesian_layer_trained.bias_posterior.mean().numpy()\n","    posterior_stddev_trained = bayesian_layer_trained.bias_posterior.stddev().numpy()\n","\n","    legend = [f\"Filter {i}\" for i in range(posterior_mean_init.shape[0])]\n","    color_iter = iter(colors)\n","\n","    for b, (bias_posterior_mean_init, bias_posterior_stddev_init, bias_posterior_mean_trained,  bias_posterior_stddev_trained) in enumerate(zip(posterior_mean_init, posterior_stddev_init, posterior_mean_trained, posterior_stddev_trained)):\n","        print(f\" Filter {b}\")\n","        bias_dist_init = tfp.distributions.Normal(loc=bias_posterior_mean_init, scale=bias_posterior_stddev_init)\n","        bias_dist_trained = tfp.distributions.Normal(loc=bias_posterior_mean_trained, scale=bias_posterior_stddev_trained)\n","\n","        _ = sns.histplot(bias_dist_init.sample(10000), ax=axs3[l,b], bins=50, color=\"b\") #kde=True)\n","        _ = sns.histplot(bias_dist_trained.sample(10000), ax=axs3[l,b], bins=50, color=\"r\") #kde=True)\n","        _ = axs3[l, b].legend([\"init\", \"trained\"])\n","        _ = axs3[l, b].set_title(f\"{layer_init.name}, f{b}\", fontdict={\"fontsize\": 10})\n","\n","        c = next(color_iter)\n","        _ = axs4[l].plot(np.array([bias_posterior_mean_init, bias_posterior_mean_trained]), np.array([0,1]), color=c)\n","        _ = axs4[l].scatter(np.array(biases_trained[b]), np.array(2), color=c)\n","        _ = axs2[l,f].set_yticks([0, 1, 2])\n","        _ = axs2[l, f].set_yticklabels([\"init\", \"trained_bayesian\",\"trained\"])\n","        _ = axs4[l].set_title(f\"Layer {l}\")\n","        _ = axs4[l].legend(legend)\n","\n","        print(f\"Init posterior:  {bias_posterior_mean_init}  {bias_posterior_stddev_init}, Trained posterior:  {bias_posterior_mean_trained}  {bias_posterior_stddev_trained}\")\n","\n","print(\"Fig. 1: Samples from posterior bias distribution\")\n","print(\"Fig. 2: Shift of posterior bias means after training.\")\n","fig3.tight_layout()\n","fig4.tight_layout()\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"PL3Jn60BWxFq"},"source":["### Save model and data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5Lt9sk16WxFr"},"outputs":[],"source":["#clf.save_model()\n","clf.pickle_results()"]},{"cell_type":"markdown","metadata":{"id":"nK9TOFCdWxFt"},"source":["### Generate report"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WKra_-rCWxFt"},"outputs":[],"source":["dir_name = clf.time + \"_\" + clf.exp_name\n","rep = clfutils.Report(dir_name)\n","rep.write()"]},{"cell_type":"markdown","metadata":{"id":"jZuYfPhUIOTO"},"source":["## Prepare website upload"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"83dFzkLu3tkL"},"outputs":[],"source":["from xpsdeeplearning.network.prepare_upload import Uploader\n","\n","dataset_path = clf.logging.hyperparams[\"input_filepath\"].rsplit(\".\",1)[0] + \"_metadata.json\"\n","uploader = Uploader(clf.logging.root_dir, dataset_path)\n","uploader.prepare_upload_params()\n","uploader.save_upload_params()"]},{"cell_type":"markdown","metadata":{"id":"Pk-dt26OLwHE"},"source":["## Save output of notebook"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mkSdyElDLwHF"},"outputs":[],"source":["from IPython.display import Javascript, display\n","from nbconvert import HTMLExporter\n","\n","def save_notebook():\n","    display(Javascript(\"IPython.notebook.save_notebook()\"),\n","            include=[\"application/javascript\"])\n","\n","def output_HTML(read_file, output_file):\n","    import codecs\n","    import nbformat\n","    exporter = HTMLExporter()\n","    # read_file is *.ipynb, output_file is *.html\n","    output_notebook = nbformat.read(read_file, as_version=4)\n","    output, resources = exporter.from_notebook_node(output_notebook)\n","    codecs.open(output_file, \"w\", encoding=\"utf-8\").write(output)\n","\n","import time\n","import os\n","\n","time.sleep(20)\n","save_notebook()\n","print(\"Notebook saved!\")\n","time.sleep(30)\n","output_file = os.path.join(clf.logging.log_dir,\n","                           \"train_prob_out.html\")\n","output_HTML(NOTEBOOK_PATH, output_file)\n","print(\"HTML file saved!\")"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["mBXaNdCjvh36","pz1u9yX1CADi","SYtqpn7tZfa-","tq0d4P6_Zhss","0FDlB_v8VaAR","KE4tBNhL7-Bu","K4mJKWJxWllD","j1lFa5GDz9dz"],"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.0"}},"nbformat":4,"nbformat_minor":0}
